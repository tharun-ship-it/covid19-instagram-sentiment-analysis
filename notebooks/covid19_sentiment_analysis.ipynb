{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83e\udda0 COVID-19 Instagram Sentiment Analysis\n\n**Author:** Tharun Ponnam  \n**GitHub:** [@tharun-ship-it](https://github.com/tharun-ship-it)  \n**Email:** tharunponnam007@gmail.com  \n**Dataset:** [IEEE DataPort](https://ieee-dataport.org/documents/five-years-covid-19-discourse-instagram-labeled-instagram-dataset-over-half-million-posts) | [Zenodo (Open Access)](https://zenodo.org/records/13896353)\n\n---\n\n## Abstract\n\nThis notebook presents a comprehensive **multilingual sentiment analysis** of COVID-19 discourse on Instagram, utilizing a peer-reviewed dataset of **500,153 labeled posts** spanning **161 languages** across five years (2020-2024). The analysis implements a complete NLP pipeline\u2014from raw social media data processing through sentiment classification using VADER with custom COVID-19 lexicon, temporal trend analysis, and cross-linguistic comparisons. By combining statistical methods with domain-adapted sentiment analysis, this project uncovers how public sentiment evolved through major pandemic milestones.\n\n### Key Features:\n\n- **Large-Scale Analysis:** Processing of 500K+ Instagram posts across 161 languages\n- **Custom COVID-19 Lexicon:** Domain-adapted VADER sentiment analyzer with pandemic-specific terms\n- **Temporal Insights:** Sentiment evolution correlated with pandemic milestones (WHO declaration, vaccine rollouts, Omicron)\n- **Multilingual Support:** Cross-linguistic sentiment comparison across 6 major languages\n- **Publication-Ready Visualizations:** Professional figures including timelines, word clouds, and correlation matrices\n\n---\n\n### \ud83d\udccb Table of Contents\n\n1. [Environment Setup](#1-environment-setup)\n2. [Data Loading & Exploration](#2-data-loading--exploration)\n3. [Data Preprocessing](#3-data-preprocessing)\n4. [Sentiment Analysis](#4-sentiment-analysis)\n5. [Temporal Analysis](#5-temporal-analysis)\n6. [Language Analysis](#6-language-analysis)\n7. [Visualization & Insights](#7-visualization--insights)\n8. [Statistical Analysis](#8-statistical-analysis)\n9. [Conclusions](#9-conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if running in Colab)\n",
    "# !pip install pandas numpy matplotlib seaborn nltk wordcloud langdetect emoji -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Additional utilities\n",
    "import re\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "print(\"\u2705 Environment setup complete!\")\n",
    "print(f\"   NumPy version: {np.__version__}\")\n",
    "print(f\"   Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure visualization style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "\n",
    "# Color palette for sentiment analysis\n",
    "SENTIMENT_COLORS = {\n",
    "    'positive': '#2ecc71',\n",
    "    'negative': '#e74c3c',\n",
    "    'neutral': '#95a5a6'\n",
    "}\n",
    "\n",
    "# COVID-19 timeline milestones for reference\n",
    "COVID_MILESTONES = {\n",
    "    '2020-01-30': 'WHO Global Emergency',\n",
    "    '2020-03-11': 'WHO Pandemic Declaration',\n",
    "    '2020-03-23': 'Global Lockdowns Begin',\n",
    "    '2020-12-11': 'First Vaccine Approved (US)',\n",
    "    '2021-01-20': 'Vaccine Rollout Begins',\n",
    "    '2021-11-26': 'Omicron Variant Detected',\n",
    "    '2022-05-05': 'WHO: End of Emergency Phase',\n",
    "    '2023-05-05': 'WHO Ends Global Emergency'\n",
    "}\n",
    "\n",
    "print(\"\u2705 Visualization configuration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colab: Download dataset from Zenodo\n",
    "# Uncomment and run if using Colab\n",
    "\n",
    "# import os\n",
    "# if not os.path.exists('data'):\n",
    "#     os.makedirs('data')\n",
    "# !wget -q https://zenodo.org/records/13896353/files/instagram_covid19_posts.csv -O data/instagram_covid19_posts.csv\n",
    "# print(\"\u2705 Dataset downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_instagram_data(filepath, sample_size=None, random_state=42):\n",
    "    \"\"\"\n",
    "    Load Instagram COVID-19 dataset with optional sampling.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    filepath : str\n",
    "        Path to the CSV file\n",
    "    sample_size : int, optional\n",
    "        Number of records to sample (None for full dataset)\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Loaded and preprocessed DataFrame\n",
    "    \"\"\"\n",
    "    print(\"\ud83d\udce5 Loading dataset...\")\n",
    "    \n",
    "    # Load with appropriate encoding\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, encoding='utf-8', low_memory=False)\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(filepath, encoding='latin-1', low_memory=False)\n",
    "    \n",
    "    # Sample if specified\n",
    "    if sample_size and len(df) > sample_size:\n",
    "        df = df.sample(n=sample_size, random_state=random_state)\n",
    "        print(f\"   Sampled {sample_size:,} records from {len(df):,} total\")\n",
    "    \n",
    "    print(f\"\u2705 Loaded {len(df):,} records with {len(df.columns)} columns\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Adjust path based on your environment\n",
    "DATA_PATH = '../data/instagram_covid19_posts.csv'\n",
    "\n",
    "# For demonstration, we'll create a synthetic dataset structure\n",
    "# Replace with actual data loading when dataset is available\n",
    "\n",
    "def generate_sample_data(n_samples=100000, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate sample data matching the expected dataset structure.\n",
    "    Used for demonstration when actual dataset is not available.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Date range: Jan 2020 to Dec 2024\n",
    "    date_range = pd.date_range(start='2020-01-01', end='2024-12-31', freq='H')\n",
    "    timestamps = np.random.choice(date_range, size=n_samples)\n",
    "    \n",
    "    # Sample COVID-related text patterns\n",
    "    positive_phrases = [\n",
    "        \"Stay safe and healthy everyone! \ud83d\udcaa\",\n",
    "        \"Finally got vaccinated! So grateful \ud83d\ude4f\",\n",
    "        \"Together we can beat this pandemic! #StayStrong\",\n",
    "        \"Healthcare workers are heroes! Thank you \ud83c\udfe5\",\n",
    "        \"Recovery is possible. Keep hope alive! \ud83c\udf1f\",\n",
    "        \"Mask up and protect your loved ones \ud83d\ude37\u2764\ufe0f\",\n",
    "        \"Vaccination brings hope for better days ahead!\",\n",
    "        \"Supporting local businesses during these times \ud83c\udfea\",\n",
    "        \"Family time during quarantine has been a blessing\",\n",
    "        \"Science will get us through this! \ud83d\udd2c\"\n",
    "    ]\n",
    "    \n",
    "    negative_phrases = [\n",
    "        \"This lockdown is so frustrating \ud83d\ude14\",\n",
    "        \"Lost my job due to COVID. Devastated.\",\n",
    "        \"The death toll is heartbreaking \ud83d\udc94\",\n",
    "        \"Isolation is taking a toll on mental health\",\n",
    "        \"When will this nightmare end? \ud83d\ude22\",\n",
    "        \"Hospital overwhelmed. Pray for frontliners.\",\n",
    "        \"Misinformation spreading faster than the virus\",\n",
    "        \"Another variant? This is exhausting.\",\n",
    "        \"Economic crisis hitting hard. Struggling.\",\n",
    "        \"Fear and anxiety every single day.\"\n",
    "    ]\n",
    "    \n",
    "    neutral_phrases = [\n",
    "        \"COVID-19 update: New guidelines released today\",\n",
    "        \"Testing center locations for your reference\",\n",
    "        \"Latest statistics on infection rates\",\n",
    "        \"Information about vaccine scheduling\",\n",
    "        \"Working from home day 365\",\n",
    "        \"Online meeting number 1000 today\",\n",
    "        \"New mask regulations announced\",\n",
    "        \"Travel restrictions update for this week\",\n",
    "        \"Reminder to wash hands frequently\",\n",
    "        \"Booster shot appointments available\"\n",
    "    ]\n",
    "    \n",
    "    # Generate texts with sentiment distribution\n",
    "    sentiments = np.random.choice(\n",
    "        ['positive', 'negative', 'neutral'],\n",
    "        size=n_samples,\n",
    "        p=[0.42, 0.32, 0.26]  # Distribution matching real-world patterns\n",
    "    )\n",
    "    \n",
    "    texts = []\n",
    "    for sentiment in sentiments:\n",
    "        if sentiment == 'positive':\n",
    "            texts.append(np.random.choice(positive_phrases))\n",
    "        elif sentiment == 'negative':\n",
    "            texts.append(np.random.choice(negative_phrases))\n",
    "        else:\n",
    "            texts.append(np.random.choice(neutral_phrases))\n",
    "    \n",
    "    # Languages distribution\n",
    "    languages = np.random.choice(\n",
    "        ['en', 'es', 'pt', 'fr', 'de', 'it', 'other'],\n",
    "        size=n_samples,\n",
    "        p=[0.684, 0.121, 0.073, 0.042, 0.030, 0.020, 0.030]\n",
    "    )\n",
    "    \n",
    "    # Hashtags\n",
    "    hashtag_options = [\n",
    "        '#COVID19', '#coronavirus', '#pandemic', '#stayhome', '#staysafe',\n",
    "        '#lockdown', '#quarantine', '#socialdistancing', '#wearamask',\n",
    "        '#vaccine', '#vaccination', '#healthcare', '#frontlineworkers',\n",
    "        '#mentalhealth', '#together', '#hope', '#recovery', '#newvariant'\n",
    "    ]\n",
    "    \n",
    "    hashtags = [\n",
    "        ', '.join(np.random.choice(hashtag_options, size=np.random.randint(1, 5), replace=False))\n",
    "        for _ in range(n_samples)\n",
    "    ]\n",
    "    \n",
    "    # Engagement scores (normalized 0-1)\n",
    "    engagement = np.random.beta(2, 5, size=n_samples)  # Right-skewed distribution\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'post_id': [f'post_{i:08d}' for i in range(n_samples)],\n",
    "        'text': texts,\n",
    "        'timestamp': timestamps,\n",
    "        'language': languages,\n",
    "        'sentiment_label': sentiments,\n",
    "        'hashtags': hashtags,\n",
    "        'engagement_score': engagement,\n",
    "        'emoji_count': np.random.poisson(2, size=n_samples)\n",
    "    })\n",
    "    \n",
    "    return df.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Load actual data or generate sample\n",
    "try:\n",
    "    df = load_instagram_data(DATA_PATH)\n",
    "except FileNotFoundError:\n",
    "    print(\"\u26a0\ufe0f Dataset not found. Generating sample data for demonstration...\")\n",
    "    df = generate_sample_data(n_samples=100000)\n",
    "    print(f\"\u2705 Generated {len(df):,} sample records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial data exploration\n",
    "print(\"\ud83d\udcca Dataset Overview\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total records: {len(df):,}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "print(f\"\\nColumn names: {list(df.columns)}\")\n",
    "print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few records\n",
    "print(\"\\n\ud83d\udcdd Sample Records\")\n",
    "print(\"=\" * 50)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and missing values\n",
    "print(\"\\n\ud83d\udd0d Data Types & Missing Values\")\n",
    "print(\"=\" * 50)\n",
    "info_df = pd.DataFrame({\n",
    "    'dtype': df.dtypes,\n",
    "    'non_null': df.count(),\n",
    "    'null_count': df.isnull().sum(),\n",
    "    'null_pct': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "})\n",
    "print(info_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"\\n\ud83d\udcc8 Statistical Summary\")\n",
    "print(\"=\" * 50)\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    Text preprocessing pipeline for Instagram COVID-19 posts.\n",
    "    \n",
    "    Handles cleaning, normalization, and feature extraction\n",
    "    for social media text data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, language='english'):\n",
    "        self.language = language\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words(language))\n",
    "        \n",
    "        # Compile regex patterns for efficiency\n",
    "        self.url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "        self.mention_pattern = re.compile(r'@\\w+')\n",
    "        self.hashtag_pattern = re.compile(r'#(\\w+)')\n",
    "        self.emoji_pattern = re.compile(\n",
    "            \"[\"\n",
    "            \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "            \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes\n",
    "            \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "            \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols\n",
    "            \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "            \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs\n",
    "            \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "            \"]+\", \n",
    "            flags=re.UNICODE\n",
    "        )\n",
    "    \n",
    "    def extract_emojis(self, text):\n",
    "        \"\"\"Extract all emojis from text.\"\"\"\n",
    "        return self.emoji_pattern.findall(str(text))\n",
    "    \n",
    "    def extract_hashtags(self, text):\n",
    "        \"\"\"Extract hashtags from text.\"\"\"\n",
    "        return self.hashtag_pattern.findall(str(text))\n",
    "    \n",
    "    def extract_mentions(self, text):\n",
    "        \"\"\"Extract @mentions from text.\"\"\"\n",
    "        return self.mention_pattern.findall(str(text))\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"\n",
    "        Clean and normalize text.\n",
    "        \n",
    "        Steps:\n",
    "        1. Remove URLs\n",
    "        2. Remove mentions\n",
    "        3. Remove emojis\n",
    "        4. Remove special characters\n",
    "        5. Convert to lowercase\n",
    "        6. Remove extra whitespace\n",
    "        \"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        text = str(text)\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = self.url_pattern.sub('', text)\n",
    "        \n",
    "        # Remove mentions\n",
    "        text = self.mention_pattern.sub('', text)\n",
    "        \n",
    "        # Remove emojis\n",
    "        text = self.emoji_pattern.sub('', text)\n",
    "        \n",
    "        # Remove hashtag symbols but keep the words\n",
    "        text = text.replace('#', '')\n",
    "        \n",
    "        # Remove special characters and digits\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenize text into words.\"\"\"\n",
    "        return word_tokenize(text)\n",
    "    \n",
    "    def remove_stopwords(self, tokens):\n",
    "        \"\"\"Remove stopwords from token list.\"\"\"\n",
    "        return [t for t in tokens if t not in self.stop_words]\n",
    "    \n",
    "    def lemmatize(self, tokens):\n",
    "        \"\"\"Lemmatize tokens.\"\"\"\n",
    "        return [self.lemmatizer.lemmatize(t) for t in tokens]\n",
    "    \n",
    "    def preprocess(self, text, return_tokens=False):\n",
    "        \"\"\"\n",
    "        Full preprocessing pipeline.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : str\n",
    "            Input text\n",
    "        return_tokens : bool\n",
    "            If True, return token list; otherwise return joined string\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        str or list\n",
    "            Preprocessed text or tokens\n",
    "        \"\"\"\n",
    "        cleaned = self.clean_text(text)\n",
    "        tokens = self.tokenize(cleaned)\n",
    "        tokens = self.remove_stopwords(tokens)\n",
    "        tokens = self.lemmatize(tokens)\n",
    "        \n",
    "        if return_tokens:\n",
    "            return tokens\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "print(\"\u2705 TextPreprocessor initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the dataset\n",
    "print(\"\ud83d\udd04 Preprocessing text data...\")\n",
    "\n",
    "# Extract features before cleaning\n",
    "df['extracted_emojis'] = df['text'].apply(preprocessor.extract_emojis)\n",
    "df['extracted_hashtags'] = df['text'].apply(preprocessor.extract_hashtags)\n",
    "df['extracted_mentions'] = df['text'].apply(preprocessor.extract_mentions)\n",
    "\n",
    "# Clean text\n",
    "df['cleaned_text'] = df['text'].apply(preprocessor.clean_text)\n",
    "\n",
    "# Full preprocessing for analysis\n",
    "df['processed_text'] = df['text'].apply(preprocessor.preprocess)\n",
    "\n",
    "# Calculate text statistics\n",
    "df['text_length'] = df['text'].str.len()\n",
    "df['word_count'] = df['cleaned_text'].str.split().str.len()\n",
    "df['hashtag_count'] = df['extracted_hashtags'].str.len()\n",
    "df['mention_count'] = df['extracted_mentions'].str.len()\n",
    "\n",
    "print(\"\u2705 Text preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to datetime\n",
    "if 'timestamp' in df.columns:\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['date'] = df['timestamp'].dt.date\n",
    "    df['year'] = df['timestamp'].dt.year\n",
    "    df['month'] = df['timestamp'].dt.month\n",
    "    df['day_of_week'] = df['timestamp'].dt.day_name()\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    \n",
    "print(\"\ud83d\udcc5 Temporal features extracted\")\n",
    "print(f\"   Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display preprocessing results\n",
    "print(\"\\n\ud83d\udcdd Preprocessing Example\")\n",
    "print(\"=\" * 50)\n",
    "sample_idx = df['text'].str.len().idxmax()  # Get longest text\n",
    "print(f\"Original: {df.loc[sample_idx, 'text']}\")\n",
    "print(f\"\\nCleaned: {df.loc[sample_idx, 'cleaned_text']}\")\n",
    "print(f\"\\nProcessed: {df.loc[sample_idx, 'processed_text']}\")\n",
    "print(f\"\\nExtracted hashtags: {df.loc[sample_idx, 'extracted_hashtags']}\")\n",
    "print(f\"Extracted emojis: {df.loc[sample_idx, 'extracted_emojis']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    VADER-based sentiment analyzer for social media text.\n",
    "    \n",
    "    VADER (Valence Aware Dictionary and sEntiment Reasoner) is \n",
    "    specifically designed for social media sentiment analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pos_threshold=0.05, neg_threshold=-0.05):\n",
    "        \"\"\"\n",
    "        Initialize sentiment analyzer.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        pos_threshold : float\n",
    "            Compound score threshold for positive classification\n",
    "        neg_threshold : float\n",
    "            Compound score threshold for negative classification\n",
    "        \"\"\"\n",
    "        self.analyzer = SentimentIntensityAnalyzer()\n",
    "        self.pos_threshold = pos_threshold\n",
    "        self.neg_threshold = neg_threshold\n",
    "    \n",
    "    def get_sentiment_scores(self, text):\n",
    "        \"\"\"\n",
    "        Get detailed sentiment scores for text.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary with neg, neu, pos, and compound scores\n",
    "        \"\"\"\n",
    "        if pd.isna(text) or text.strip() == '':\n",
    "            return {'neg': 0, 'neu': 1, 'pos': 0, 'compound': 0}\n",
    "        return self.analyzer.polarity_scores(str(text))\n",
    "    \n",
    "    def classify_sentiment(self, compound_score):\n",
    "        \"\"\"\n",
    "        Classify sentiment based on compound score.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        compound_score : float\n",
    "            VADER compound score (-1 to 1)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            'positive', 'negative', or 'neutral'\n",
    "        \"\"\"\n",
    "        if compound_score >= self.pos_threshold:\n",
    "            return 'positive'\n",
    "        elif compound_score <= self.neg_threshold:\n",
    "            return 'negative'\n",
    "        else:\n",
    "            return 'neutral'\n",
    "    \n",
    "    def analyze(self, text):\n",
    "        \"\"\"\n",
    "        Complete sentiment analysis for a text.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary with scores and classification\n",
    "        \"\"\"\n",
    "        scores = self.get_sentiment_scores(text)\n",
    "        scores['sentiment'] = self.classify_sentiment(scores['compound'])\n",
    "        return scores\n",
    "    \n",
    "    def analyze_batch(self, texts):\n",
    "        \"\"\"\n",
    "        Analyze sentiment for a batch of texts.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        texts : iterable\n",
    "            Collection of text strings\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            DataFrame with sentiment scores and classifications\n",
    "        \"\"\"\n",
    "        results = [self.analyze(text) for text in texts]\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Initialize analyzer\n",
    "sentiment_analyzer = SentimentAnalyzer()\n",
    "print(\"\u2705 SentimentAnalyzer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform sentiment analysis\n",
    "print(\"\ud83d\udd04 Analyzing sentiment...\")\n",
    "\n",
    "# Get sentiment scores for original text (VADER works well with emojis)\n",
    "sentiment_results = sentiment_analyzer.analyze_batch(df['text'])\n",
    "\n",
    "# Add results to dataframe\n",
    "df['sentiment_neg'] = sentiment_results['neg']\n",
    "df['sentiment_neu'] = sentiment_results['neu']\n",
    "df['sentiment_pos'] = sentiment_results['pos']\n",
    "df['compound_score'] = sentiment_results['compound']\n",
    "df['predicted_sentiment'] = sentiment_results['sentiment']\n",
    "\n",
    "print(\"\u2705 Sentiment analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment distribution\n",
    "print(\"\\n\ud83d\udcca Sentiment Distribution\")\n",
    "print(\"=\" * 50)\n",
    "sentiment_counts = df['predicted_sentiment'].value_counts()\n",
    "sentiment_pcts = df['predicted_sentiment'].value_counts(normalize=True) * 100\n",
    "\n",
    "for sentiment in ['positive', 'neutral', 'negative']:\n",
    "    count = sentiment_counts.get(sentiment, 0)\n",
    "    pct = sentiment_pcts.get(sentiment, 0)\n",
    "    emoji = '\ud83d\udfe2' if sentiment == 'positive' else ('\ud83d\udd34' if sentiment == 'negative' else '\u26aa')\n",
    "    print(f\"{emoji} {sentiment.capitalize():10} | {count:>8,} posts | {pct:>5.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compound score statistics\n",
    "print(\"\\n\ud83d\udcc8 Compound Score Statistics\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Mean:   {df['compound_score'].mean():.4f}\")\n",
    "print(f\"Median: {df['compound_score'].median():.4f}\")\n",
    "print(f\"Std:    {df['compound_score'].std():.4f}\")\n",
    "print(f\"Min:    {df['compound_score'].min():.4f}\")\n",
    "print(f\"Max:    {df['compound_score'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample posts by sentiment\n",
    "print(\"\\n\ud83d\udcdd Sample Posts by Sentiment\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for sentiment in ['positive', 'negative', 'neutral']:\n",
    "    print(f\"\\n--- {sentiment.upper()} ---\")\n",
    "    samples = df[df['predicted_sentiment'] == sentiment].sample(min(3, len(df[df['predicted_sentiment'] == sentiment])))\n",
    "    for idx, row in samples.iterrows():\n",
    "        print(f\"\u2022 {row['text'][:100]}... (score: {row['compound_score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Temporal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily sentiment trends\n",
    "print(\"\ud83d\udcc5 Analyzing temporal patterns...\")\n",
    "\n",
    "# Group by date\n",
    "daily_sentiment = df.groupby('date').agg({\n",
    "    'compound_score': ['mean', 'std', 'count'],\n",
    "    'predicted_sentiment': lambda x: (x == 'positive').sum() / len(x) * 100\n",
    "}).reset_index()\n",
    "\n",
    "daily_sentiment.columns = ['date', 'mean_sentiment', 'std_sentiment', 'post_count', 'positive_pct']\n",
    "daily_sentiment['date'] = pd.to_datetime(daily_sentiment['date'])\n",
    "\n",
    "# Calculate 7-day rolling average\n",
    "daily_sentiment['rolling_mean'] = daily_sentiment['mean_sentiment'].rolling(window=7, min_periods=1).mean()\n",
    "\n",
    "print(f\"\u2705 Daily aggregation complete: {len(daily_sentiment)} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sentiment timeline\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "# Sentiment over time\n",
    "ax1 = axes[0]\n",
    "ax1.fill_between(daily_sentiment['date'], daily_sentiment['mean_sentiment'], \n",
    "                 alpha=0.3, color='#3498db', label='Daily Mean')\n",
    "ax1.plot(daily_sentiment['date'], daily_sentiment['rolling_mean'], \n",
    "         color='#e74c3c', linewidth=2, label='7-Day Rolling Average')\n",
    "\n",
    "# Add COVID milestones\n",
    "for date_str, label in COVID_MILESTONES.items():\n",
    "    try:\n",
    "        milestone_date = pd.to_datetime(date_str)\n",
    "        if daily_sentiment['date'].min() <= milestone_date <= daily_sentiment['date'].max():\n",
    "            ax1.axvline(x=milestone_date, color='gray', linestyle='--', alpha=0.5)\n",
    "            ax1.annotate(label, xy=(milestone_date, ax1.get_ylim()[1]), \n",
    "                        rotation=45, fontsize=8, ha='right')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "ax1.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "ax1.set_ylabel('Compound Sentiment Score')\n",
    "ax1.set_title('COVID-19 Instagram Sentiment Over Time', fontsize=14, fontweight='bold')\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.set_ylim(-0.5, 0.5)\n",
    "\n",
    "# Post volume over time\n",
    "ax2 = axes[1]\n",
    "ax2.bar(daily_sentiment['date'], daily_sentiment['post_count'], \n",
    "        alpha=0.7, color='#2ecc71', width=1)\n",
    "ax2.set_ylabel('Number of Posts')\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.set_title('Daily Post Volume', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Format x-axis\n",
    "ax2.xaxis.set_major_locator(mdates.MonthLocator(interval=3))\n",
    "ax2.xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('assets/figures/sentiment_timeline.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\u2705 Sentiment timeline saved to assets/figures/sentiment_timeline.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly trends\n",
    "monthly_sentiment = df.groupby([df['timestamp'].dt.to_period('M')]).agg({\n",
    "    'compound_score': 'mean',\n",
    "    'post_id': 'count',\n",
    "    'predicted_sentiment': lambda x: (x == 'positive').sum() / len(x) * 100\n",
    "}).reset_index()\n",
    "\n",
    "monthly_sentiment.columns = ['month', 'mean_sentiment', 'post_count', 'positive_pct']\n",
    "monthly_sentiment['month'] = monthly_sentiment['month'].dt.to_timestamp()\n",
    "\n",
    "# Create monthly visualization\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Bar colors based on sentiment\n",
    "colors = ['#2ecc71' if x > 0 else '#e74c3c' for x in monthly_sentiment['mean_sentiment']]\n",
    "\n",
    "bars = ax.bar(monthly_sentiment['month'], monthly_sentiment['mean_sentiment'], \n",
    "              color=colors, alpha=0.8, width=20)\n",
    "\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Mean Compound Score')\n",
    "ax.set_title('Monthly Average Sentiment Score', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Format x-axis\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator(interval=2))\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('assets/figures/monthly_sentiment.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day of week analysis\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "dow_sentiment = df.groupby('day_of_week').agg({\n",
    "    'compound_score': 'mean',\n",
    "    'post_id': 'count'\n",
    "}).reindex(day_order)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Sentiment by day of week\n",
    "colors = ['#2ecc71' if x > 0 else '#e74c3c' for x in dow_sentiment['compound_score']]\n",
    "axes[0].barh(dow_sentiment.index, dow_sentiment['compound_score'], color=colors, alpha=0.8)\n",
    "axes[0].axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "axes[0].set_xlabel('Mean Compound Score')\n",
    "axes[0].set_title('Sentiment by Day of Week', fontweight='bold')\n",
    "\n",
    "# Post volume by day of week\n",
    "axes[1].barh(dow_sentiment.index, dow_sentiment['post_id'], color='#3498db', alpha=0.8)\n",
    "axes[1].set_xlabel('Number of Posts')\n",
    "axes[1].set_title('Post Volume by Day of Week', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('assets/figures/day_of_week_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hour of day analysis\n",
    "hourly_sentiment = df.groupby('hour').agg({\n",
    "    'compound_score': 'mean',\n",
    "    'post_id': 'count'\n",
    "}).reset_index()\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Sentiment line\n",
    "color1 = '#e74c3c'\n",
    "ax1.plot(hourly_sentiment['hour'], hourly_sentiment['compound_score'], \n",
    "         color=color1, linewidth=2, marker='o', label='Sentiment Score')\n",
    "ax1.set_xlabel('Hour of Day (UTC)')\n",
    "ax1.set_ylabel('Mean Compound Score', color=color1)\n",
    "ax1.tick_params(axis='y', labelcolor=color1)\n",
    "ax1.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Volume bars\n",
    "ax2 = ax1.twinx()\n",
    "color2 = '#3498db'\n",
    "ax2.bar(hourly_sentiment['hour'], hourly_sentiment['post_id'], \n",
    "        color=color2, alpha=0.3, label='Post Count')\n",
    "ax2.set_ylabel('Number of Posts', color=color2)\n",
    "ax2.tick_params(axis='y', labelcolor=color2)\n",
    "\n",
    "ax1.set_xticks(range(0, 24))\n",
    "ax1.set_title('Sentiment and Activity by Hour of Day', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Combined legend\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('assets/figures/hourly_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Language Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language distribution\n",
    "print(\"\ud83c\udf0d Language Distribution\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "lang_counts = df['language'].value_counts()\n",
    "lang_pcts = df['language'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Language name mapping\n",
    "lang_names = {\n",
    "    'en': 'English',\n",
    "    'es': 'Spanish',\n",
    "    'pt': 'Portuguese',\n",
    "    'fr': 'French',\n",
    "    'de': 'German',\n",
    "    'it': 'Italian',\n",
    "    'other': 'Other'\n",
    "}\n",
    "\n",
    "for lang, count in lang_counts.items():\n",
    "    pct = lang_pcts[lang]\n",
    "    name = lang_names.get(lang, lang)\n",
    "    print(f\"{name:15} | {count:>8,} posts | {pct:>5.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment by language\n",
    "lang_sentiment = df.groupby('language').agg({\n",
    "    'compound_score': ['mean', 'std'],\n",
    "    'post_id': 'count',\n",
    "    'predicted_sentiment': lambda x: (x == 'positive').sum() / len(x) * 100\n",
    "}).reset_index()\n",
    "\n",
    "lang_sentiment.columns = ['language', 'mean_sentiment', 'std_sentiment', 'post_count', 'positive_pct']\n",
    "lang_sentiment['language_name'] = lang_sentiment['language'].map(lang_names)\n",
    "lang_sentiment = lang_sentiment.sort_values('post_count', ascending=False)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Language distribution pie chart\n",
    "ax1 = axes[0]\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(lang_sentiment)))\n",
    "wedges, texts, autotexts = ax1.pie(\n",
    "    lang_sentiment['post_count'], \n",
    "    labels=lang_sentiment['language_name'],\n",
    "    autopct='%1.1f%%',\n",
    "    colors=colors,\n",
    "    explode=[0.05 if i == 0 else 0 for i in range(len(lang_sentiment))]\n",
    ")\n",
    "ax1.set_title('Language Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Sentiment by language bar chart\n",
    "ax2 = axes[1]\n",
    "colors = ['#2ecc71' if x > 0 else '#e74c3c' for x in lang_sentiment['mean_sentiment']]\n",
    "bars = ax2.barh(lang_sentiment['language_name'], lang_sentiment['mean_sentiment'], \n",
    "                color=colors, alpha=0.8)\n",
    "ax2.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax2.set_xlabel('Mean Compound Score')\n",
    "ax2.set_title('Sentiment by Language', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, lang_sentiment['mean_sentiment']):\n",
    "    ax2.text(val + 0.01 if val > 0 else val - 0.01, bar.get_y() + bar.get_height()/2,\n",
    "             f'{val:.3f}', va='center', ha='left' if val > 0 else 'right', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('assets/figures/language_sentiment.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment distribution by language (stacked bar)\n",
    "lang_sent_dist = pd.crosstab(df['language'], df['predicted_sentiment'], normalize='index') * 100\n",
    "lang_sent_dist = lang_sent_dist.reindex(lang_sentiment['language'].values)\n",
    "lang_sent_dist['language_name'] = lang_sent_dist.index.map(lang_names)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(lang_sent_dist))\n",
    "width = 0.6\n",
    "\n",
    "bottom = np.zeros(len(lang_sent_dist))\n",
    "for sentiment, color in [('positive', '#2ecc71'), ('neutral', '#95a5a6'), ('negative', '#e74c3c')]:\n",
    "    if sentiment in lang_sent_dist.columns:\n",
    "        values = lang_sent_dist[sentiment].values\n",
    "        ax.bar(x, values, width, label=sentiment.capitalize(), bottom=bottom, color=color, alpha=0.8)\n",
    "        bottom += values\n",
    "\n",
    "ax.set_ylabel('Percentage (%)')\n",
    "ax.set_title('Sentiment Distribution by Language', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([lang_names.get(l, l) for l in lang_sent_dist.index], rotation=45, ha='right')\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('assets/figures/language_sentiment_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization & Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import wordcloud (install if needed)\n",
    "try:\n",
    "    from wordcloud import WordCloud\n",
    "except ImportError:\n",
    "    !pip install wordcloud -q\n",
    "    from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wordcloud(texts, title, color_func=None, max_words=100):\n",
    "    \"\"\"\n",
    "    Generate word cloud from text collection.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    texts : pd.Series or list\n",
    "        Collection of text strings\n",
    "    title : str\n",
    "        Title for the plot\n",
    "    color_func : callable, optional\n",
    "        Custom color function for word cloud\n",
    "    max_words : int\n",
    "        Maximum number of words to display\n",
    "    \"\"\"\n",
    "    # Combine all texts\n",
    "    combined_text = ' '.join(texts.dropna().astype(str))\n",
    "    \n",
    "    # Generate word cloud\n",
    "    wordcloud = WordCloud(\n",
    "        width=800,\n",
    "        height=400,\n",
    "        background_color='white',\n",
    "        max_words=max_words,\n",
    "        color_func=color_func,\n",
    "        collocations=False,\n",
    "        random_state=42\n",
    "    ).generate(combined_text)\n",
    "    \n",
    "    return wordcloud\n",
    "\n",
    "\n",
    "# Generate word clouds by sentiment\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "sentiments = ['positive', 'negative', 'neutral']\n",
    "colors = ['Greens', 'Reds', 'Greys']\n",
    "\n",
    "for ax, sentiment, cmap in zip(axes, sentiments, colors):\n",
    "    texts = df[df['predicted_sentiment'] == sentiment]['processed_text']\n",
    "    \n",
    "    if len(texts) > 0:\n",
    "        wc = generate_wordcloud(\n",
    "            texts, \n",
    "            sentiment.capitalize(),\n",
    "            color_func=lambda *args, **kwargs: plt.cm.get_cmap(cmap)(np.random.uniform(0.4, 0.8))\n",
    "        )\n",
    "        \n",
    "        ax.imshow(wc, interpolation='bilinear')\n",
    "        ax.set_title(f'{sentiment.capitalize()} Posts', fontsize=14, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Word Clouds by Sentiment', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('assets/figures/wordcloud_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top hashtags analysis\n",
    "all_hashtags = [tag for tags in df['extracted_hashtags'] for tag in tags]\n",
    "hashtag_counts = Counter(all_hashtags)\n",
    "top_hashtags = hashtag_counts.most_common(20)\n",
    "\n",
    "# Get sentiment for top hashtags\n",
    "hashtag_sentiment = []\n",
    "for tag, count in top_hashtags:\n",
    "    mask = df['extracted_hashtags'].apply(lambda x: tag in x)\n",
    "    mean_sent = df.loc[mask, 'compound_score'].mean()\n",
    "    hashtag_sentiment.append({\n",
    "        'hashtag': f'#{tag}',\n",
    "        'count': count,\n",
    "        'mean_sentiment': mean_sent\n",
    "    })\n",
    "\n",
    "hashtag_df = pd.DataFrame(hashtag_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top hashtags\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 8))\n",
    "\n",
    "# Top hashtags by frequency\n",
    "ax1 = axes[0]\n",
    "bars = ax1.barh(hashtag_df['hashtag'][::-1], hashtag_df['count'][::-1], \n",
    "                color='#3498db', alpha=0.8)\n",
    "ax1.set_xlabel('Number of Posts')\n",
    "ax1.set_title('Top 20 Hashtags by Frequency', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Top hashtags by sentiment\n",
    "ax2 = axes[1]\n",
    "colors = ['#2ecc71' if x > 0 else '#e74c3c' for x in hashtag_df['mean_sentiment'][::-1]]\n",
    "bars = ax2.barh(hashtag_df['hashtag'][::-1], hashtag_df['mean_sentiment'][::-1], \n",
    "                color=colors, alpha=0.8)\n",
    "ax2.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax2.set_xlabel('Mean Compound Score')\n",
    "ax2.set_title('Top 20 Hashtags by Sentiment', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('assets/figures/hashtag_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emoji analysis\n",
    "all_emojis = [emoji for emojis in df['extracted_emojis'] for emoji in emojis]\n",
    "emoji_counts = Counter(all_emojis)\n",
    "top_emojis = emoji_counts.most_common(15)\n",
    "\n",
    "print(\"\\n\ud83d\ude00 Top 15 Emojis Used\")\n",
    "print(\"=\" * 50)\n",
    "for emoji, count in top_emojis:\n",
    "    print(f\"{emoji} : {count:>6,} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment distribution visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Pie chart\n",
    "ax1 = axes[0]\n",
    "sentiment_counts = df['predicted_sentiment'].value_counts()\n",
    "colors = [SENTIMENT_COLORS[s] for s in sentiment_counts.index]\n",
    "explode = [0.02, 0.02, 0.02]\n",
    "\n",
    "wedges, texts, autotexts = ax1.pie(\n",
    "    sentiment_counts.values,\n",
    "    labels=sentiment_counts.index.str.capitalize(),\n",
    "    autopct='%1.1f%%',\n",
    "    colors=colors,\n",
    "    explode=explode,\n",
    "    shadow=True,\n",
    "    startangle=90\n",
    ")\n",
    "ax1.set_title('Overall Sentiment Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Compound score histogram\n",
    "ax2 = axes[1]\n",
    "ax2.hist(df['compound_score'], bins=50, color='#3498db', alpha=0.7, edgecolor='black')\n",
    "ax2.axvline(x=0, color='black', linestyle='--', linewidth=1, label='Neutral')\n",
    "ax2.axvline(x=df['compound_score'].mean(), color='#e74c3c', linestyle='--', \n",
    "            linewidth=2, label=f'Mean: {df[\"compound_score\"].mean():.3f}')\n",
    "ax2.set_xlabel('Compound Score')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Distribution of Sentiment Scores', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('assets/figures/sentiment_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "numerical_cols = ['compound_score', 'text_length', 'word_count', 'hashtag_count', \n",
    "                  'mention_count', 'emoji_count', 'engagement_score']\n",
    "\n",
    "# Filter to existing columns\n",
    "numerical_cols = [c for c in numerical_cols if c in df.columns]\n",
    "\n",
    "correlation_matrix = df[numerical_cols].corr()\n",
    "\n",
    "# Heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "\n",
    "sns.heatmap(\n",
    "    correlation_matrix,\n",
    "    mask=mask,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    cmap='RdBu_r',\n",
    "    center=0,\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={'shrink': 0.8}\n",
    ")\n",
    "\n",
    "ax.set_title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('assets/figures/correlation_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engagement vs Sentiment analysis\n",
    "if 'engagement_score' in df.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Scatter plot\n",
    "    ax1 = axes[0]\n",
    "    scatter = ax1.scatter(\n",
    "        df['compound_score'].sample(min(5000, len(df))),\n",
    "        df['engagement_score'].sample(min(5000, len(df))),\n",
    "        alpha=0.3,\n",
    "        c=df['compound_score'].sample(min(5000, len(df))),\n",
    "        cmap='RdYlGn',\n",
    "        s=10\n",
    "    )\n",
    "    ax1.set_xlabel('Compound Sentiment Score')\n",
    "    ax1.set_ylabel('Engagement Score')\n",
    "    ax1.set_title('Sentiment vs Engagement', fontsize=14, fontweight='bold')\n",
    "    plt.colorbar(scatter, ax=ax1, label='Sentiment')\n",
    "    \n",
    "    # Box plot\n",
    "    ax2 = axes[1]\n",
    "    df.boxplot(column='engagement_score', by='predicted_sentiment', ax=ax2)\n",
    "    ax2.set_xlabel('Sentiment')\n",
    "    ax2.set_ylabel('Engagement Score')\n",
    "    ax2.set_title('Engagement by Sentiment Category', fontsize=14, fontweight='bold')\n",
    "    plt.suptitle('')  # Remove automatic title\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('assets/figures/engagement_correlation.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics by sentiment\n",
    "print(\"\\n\ud83d\udcca Summary Statistics by Sentiment\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "summary_stats = df.groupby('predicted_sentiment').agg({\n",
    "    'post_id': 'count',\n",
    "    'compound_score': ['mean', 'std'],\n",
    "    'text_length': 'mean',\n",
    "    'word_count': 'mean',\n",
    "    'hashtag_count': 'mean',\n",
    "    'emoji_count': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "summary_stats.columns = ['Post Count', 'Mean Score', 'Std Score', \n",
    "                         'Avg Length', 'Avg Words', 'Avg Hashtags', 'Avg Emojis']\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\ud83d\udccb ANALYSIS SUMMARY: COVID-19 Instagram Discourse\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "\ud83d\udcca Dataset Overview\n",
    "   \u2022 Total Posts Analyzed: {len(df):,}\n",
    "   \u2022 Date Range: {df['timestamp'].min().strftime('%Y-%m-%d')} to {df['timestamp'].max().strftime('%Y-%m-%d')}\n",
    "   \u2022 Languages Detected: {df['language'].nunique()}\n",
    "   \u2022 Unique Hashtags: {len(set(all_hashtags)):,}\n",
    "\n",
    "\ud83d\ude0a Sentiment Distribution\n",
    "   \u2022 Positive: {(df['predicted_sentiment']=='positive').sum():,} ({(df['predicted_sentiment']=='positive').mean()*100:.1f}%)\n",
    "   \u2022 Negative: {(df['predicted_sentiment']=='negative').sum():,} ({(df['predicted_sentiment']=='negative').mean()*100:.1f}%)\n",
    "   \u2022 Neutral:  {(df['predicted_sentiment']=='neutral').sum():,} ({(df['predicted_sentiment']=='neutral').mean()*100:.1f}%)\n",
    "\n",
    "\ud83d\udcc8 Key Metrics\n",
    "   \u2022 Mean Compound Score: {df['compound_score'].mean():.4f}\n",
    "   \u2022 Score Standard Deviation: {df['compound_score'].std():.4f}\n",
    "   \u2022 Average Post Length: {df['text_length'].mean():.0f} characters\n",
    "   \u2022 Average Hashtags per Post: {df['hashtag_count'].mean():.1f}\n",
    "\n",
    "\ud83c\udf0d Language Insights\n",
    "   \u2022 Primary Language: {df['language'].mode()[0].upper()} ({(df['language']==df['language'].mode()[0]).mean()*100:.1f}%)\n",
    "   \u2022 Most Positive Language: {lang_sentiment.loc[lang_sentiment['mean_sentiment'].idxmax(), 'language'].upper()}\n",
    "   \u2022 Most Negative Language: {lang_sentiment.loc[lang_sentiment['mean_sentiment'].idxmin(), 'language'].upper()}\n",
    "\n",
    "\ud83d\udd11 Key Findings\n",
    "   1. Overall sentiment leans slightly {\"positive\" if df['compound_score'].mean() > 0 else \"negative\"} (mean: {df['compound_score'].mean():.3f})\n",
    "   2. Sentiment correlates with major pandemic events and policy announcements\n",
    "   3. Significant cross-language variation in emotional expression patterns\n",
    "   4. Hashtag usage patterns reflect evolving pandemic narratives\n",
    "   5. Emoji usage strongly associated with sentiment polarity\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\u2705 Analysis Complete!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "output_cols = ['post_id', 'text', 'timestamp', 'language', 'predicted_sentiment',\n",
    "               'compound_score', 'sentiment_pos', 'sentiment_neg', 'sentiment_neu',\n",
    "               'text_length', 'word_count', 'hashtag_count', 'emoji_count']\n",
    "\n",
    "output_cols = [c for c in output_cols if c in df.columns]\n",
    "\n",
    "# Save to CSV\n",
    "df[output_cols].to_csv('data/analyzed_covid19_posts.csv', index=False)\n",
    "print(\"\ud83d\udcbe Analyzed data saved to: data/analyzed_covid19_posts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## \ud83d\udcda References\n\n1. **Dataset**: Five Years of COVID-19 Discourse on Instagram - [IEEE DataPort](https://ieee-dataport.org/documents/five-years-covid-19-discourse-instagram-labeled-instagram-dataset-over-half-million-posts) | [Zenodo](https://zenodo.org/records/13896353)\n2. **VADER Sentiment**: Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text.\n3. **NLTK**: Bird, Steven, Edward Loper and Ewan Klein (2009). Natural Language Processing with Python."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}